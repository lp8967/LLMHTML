import json
import logging
import os
import sys
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤ –≤ Docker
sys.path.append('/app')

from app.database import vector_db
from app.config import DATA_PATH, BATCH_SIZE

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_arxiv_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ arXiv JSON"""
    
    logger.info("=== STARTING DATA LOADING ===")
    logger.info(f"Current directory: {os.getcwd()}")
    logger.info(f"DATA_PATH: {DATA_PATH}")
    
    # üî¥ –ò–°–ü–†–ê–í–õ–Ø–ï–ú –ü–†–û–ë–õ–ï–ú–£: –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ö–û–õ–ò–ß–ï–°–¢–í–û –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    try:
        count = vector_db.collection.count()
        logger.info(f"üìä Current document count: {count}")
        
        if count > 100:  # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            logger.info(f"‚úÖ Database already has {count} documents, skipping loading.")
            return True
        elif count > 0:
            logger.warning(f"‚ö†Ô∏è Database has only {count} documents, but proceeding with loading...")
        else:
            logger.info("üîÑ Database is EMPTY - loading data...")
    except Exception as e:
        logger.warning(f"Could not check collection count: {e}")
        logger.info("Proceeding with data loading...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(DATA_PATH):
        logger.error(f"‚ùå Data file not found: {DATA_PATH}")
        logger.info(f"Files in current directory: {os.listdir('.')}")
        if os.path.exists('data'):
            logger.info(f"Files in data directory: {os.listdir('data')}")
        return False
    
    try:
        logger.info(f"üìñ Reading data from {DATA_PATH}")
        with open(DATA_PATH, 'r', encoding='utf-8') as f:
            papers = json.load(f)
        
        logger.info(f"üìä Loaded {len(papers)} papers from dataset")
        
        # –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –î–û 1000 –°–¢–ê–¢–ï–ô –î–õ–Ø –ë–´–°–¢–†–û–ô –ó–ê–ì–†–£–ó–ö–ò
        papers = papers[:101]
        logger.info(f"üì¶ Processing {len(papers)} papers")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        documents = []
        metadatas = []
        ids = []
        
        for i, paper in enumerate(tqdm(papers, desc="Processing papers")):
            # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
            text_content = create_document_text(paper)
            
            documents.append(text_content)
            metadatas.append({
                "paper_id": paper.get("id", f"unknown_{i}"),
                "title": paper.get("title", ""),
                "authors": paper.get("authors", ""),
                "categories": paper.get("categories", ""),
                "year": "2020"
            })
            ids.append(f"arxiv_{paper.get('id', i)}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            if len(documents) >= BATCH_SIZE:
                logger.info(f"üì§ Adding batch of {len(documents)} documents...")
                vector_db.add_documents(documents, metadatas, ids)
                documents.clear()
                metadatas.clear()
                ids.clear()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if documents:
            logger.info(f"üì§ Adding final batch of {len(documents)} documents...")
            vector_db.add_documents(documents, metadatas, ids)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final_count = vector_db.collection.count()
        logger.info(f"‚úÖ Successfully loaded {final_count} papers into vector database")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error loading arXiv data: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def create_document_text(paper):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    title = paper.get("title", "").strip()
    abstract = paper.get("abstract", "").strip()
    authors = paper.get("authors", "").strip()
    categories = paper.get("categories", "").strip()
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    document_text = f"Title: {title}\n"
    
    if authors:
        document_text += f"Authors: {authors}\n"
    
    if categories:
        document_text += f"Categories: {categories}\n"
    
    document_text += f"Abstract: {abstract}"
    
    return document_text

if __name__ == "__main__":
    success = process_arxiv_data()
    if success:
        print("Data loading completed successfully!")
    else:
        print("Data loading failed!")

        sys.exit(1)

